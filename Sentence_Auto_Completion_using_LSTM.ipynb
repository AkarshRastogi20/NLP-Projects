{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-KrcioznX1P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSfradtnKPSs"
      },
      "outputs": [],
      "source": [
        "# Import the data\n",
        "\n",
        "# specify the file name\n",
        "input_file='holmes.txt'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Hg9ZfmnkLj14",
        "outputId": "b3007294-58c7-43b7-fde4-4ed80cc346d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"*Project Gutenberg's Etext of Tom Swift And His Submarine Boat*\\n\\n#4 in the Victor Appleton's Tom Swi\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD43hiHcMun_"
      },
      "outputs": [],
      "source": [
        "text=text[:500000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRLYr_NkSQeU"
      },
      "outputs": [],
      "source": [
        "# Function to remove emojis and special characters from text\n",
        "def remove_emojis_and_special_characters(data):\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    # Remove special characters\n",
        "    data = re.sub(r'[^a-zA-Z0-9\\s]', '', data)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    data = re.sub(' +', ' ', data)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PoM0j-GNLFF"
      },
      "outputs": [],
      "source": [
        "# clean the data\n",
        "\n",
        "# Preprocessing pipeline\n",
        "def preprocess_pipeline(text) -> 'list':\n",
        "    # Split by newline character\n",
        "    sentences = text.split('\\n')\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = remove_emojis_and_special_characters(sentences[i])\n",
        "    # Remove leading and trailing spaces\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    # Drop empty sentences\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    # Tokenization\n",
        "    tokenized = []\n",
        "    for sentence in sentences:\n",
        "        # Convert to lowercase\n",
        "        sentence = sentence.lower()\n",
        "        tokenized.append(sentence)\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_sentences = preprocess_pipeline(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sALjfgMeRqCY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "What is an OOV Token?\n",
        "An out-of-vocabulary (OOV) token is a special token used in natural language processing (NLP) tasks to represent words that\n",
        "are not present in the vocabulary of the model or tokenizer. When a word that is not in the vocabulary is encountered during\n",
        "tokenization or text processing, it is replaced with the OOV token.\n",
        "\n",
        "Why Use an OOV Token?\n",
        "Using an OOV token helps handle unseen or unknown words during the training or inference phase of an NLP model.\n",
        "Instead of encountering errors or issues when encountering unknown words, the model can gracefully handle them by\n",
        "representing them with the OOV token. This is particularly useful when working with real-world data where the vocabulary\n",
        "of the model may not cover all possible words.\n",
        "\"\"\"\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(tokenized_sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "# tokenizer.word_counts\n",
        "# tokenizer.word_index\n",
        "\"\"\"\n",
        "n_gram example:\n",
        "[3, 15, 8, 7, 20, 12, 6]\n",
        "\n",
        "For the above sentece sentence, the code would generate the following n-gram sequences:\n",
        "\n",
        "[3, 15]\n",
        "[3, 15, 8]\n",
        "[3, 15, 8, 7]\n",
        "[3, 15, 8, 7, 20]\n",
        "[3, 15, 8, 7, 20, 12]\n",
        "[3, 15, 8, 7, 20, 12, 6]\n",
        "\"\"\"\n",
        "\n",
        "# Generate input sequences\n",
        "input_sequences = []\n",
        "for line in tokenized_sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiY1OwubTIJu"
      },
      "outputs": [],
      "source": [
        "# Creates labels with input sequences\n",
        "X,labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ZX2ePQTXQt"
      },
      "outputs": [],
      "source": [
        "# Split data into training, validation, and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_temp, X_val_test, y_train_temp, y_val_test = train_test_split(X, ys, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnmzXNlzT35U"
      },
      "source": [
        "**Train LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnAmOpMCTeyk",
        "outputId": "8cae2a34-01e4-4ac9-f108-85c80864e433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 107ms/step - accuracy: 0.0940 - loss: 6.3115 - val_accuracy: 0.1344 - val_loss: 5.6954\n",
            "Epoch 2/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 101ms/step - accuracy: 0.1560 - loss: 5.1466 - val_accuracy: 0.1498 - val_loss: 5.6197\n",
            "Epoch 3/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 99ms/step - accuracy: 0.1804 - loss: 4.6459 - val_accuracy: 0.1559 - val_loss: 5.7507\n",
            "Epoch 4/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 92ms/step - accuracy: 0.1956 - loss: 4.2696 - val_accuracy: 0.1569 - val_loss: 5.9479\n",
            "Epoch 5/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 92ms/step - accuracy: 0.2205 - loss: 3.9859 - val_accuracy: 0.1525 - val_loss: 6.1341\n",
            "Epoch 6/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 96ms/step - accuracy: 0.2377 - loss: 3.7822 - val_accuracy: 0.1561 - val_loss: 6.3994\n",
            "Epoch 7/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 94ms/step - accuracy: 0.2479 - loss: 3.6543 - val_accuracy: 0.1545 - val_loss: 6.6453\n",
            "Epoch 8/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 92ms/step - accuracy: 0.2565 - loss: 3.5617 - val_accuracy: 0.1505 - val_loss: 6.8693\n",
            "Epoch 9/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 98ms/step - accuracy: 0.2645 - loss: 3.4693 - val_accuracy: 0.1478 - val_loss: 7.0772\n",
            "Epoch 10/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 91ms/step - accuracy: 0.2743 - loss: 3.4003 - val_accuracy: 0.1490 - val_loss: 7.2439\n",
            "Epoch 11/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 91ms/step - accuracy: 0.2778 - loss: 3.3498 - val_accuracy: 0.1438 - val_loss: 7.4194\n",
            "Epoch 12/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 91ms/step - accuracy: 0.2847 - loss: 3.3152 - val_accuracy: 0.1476 - val_loss: 7.6064\n",
            "Epoch 13/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 91ms/step - accuracy: 0.2889 - loss: 3.2986 - val_accuracy: 0.1429 - val_loss: 7.7675\n",
            "Epoch 14/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 91ms/step - accuracy: 0.2916 - loss: 3.2545 - val_accuracy: 0.1426 - val_loss: 7.8941\n",
            "Epoch 15/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 91ms/step - accuracy: 0.2957 - loss: 3.2561 - val_accuracy: 0.1480 - val_loss: 8.0510\n",
            "Epoch 16/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 90ms/step - accuracy: 0.2888 - loss: 3.2964 - val_accuracy: 0.1460 - val_loss: 8.1186\n",
            "Epoch 17/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 94ms/step - accuracy: 0.2948 - loss: 3.2114 - val_accuracy: 0.1411 - val_loss: 8.2709\n",
            "Epoch 18/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 93ms/step - accuracy: 0.2965 - loss: 3.2229 - val_accuracy: 0.1435 - val_loss: 8.3812\n",
            "Epoch 19/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 94ms/step - accuracy: 0.2908 - loss: 3.3328 - val_accuracy: 0.1398 - val_loss: 8.5137\n",
            "Epoch 20/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 91ms/step - accuracy: 0.2948 - loss: 3.2389 - val_accuracy: 0.1401 - val_loss: 8.6163\n",
            "Epoch 21/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 94ms/step - accuracy: 0.2951 - loss: 3.2201 - val_accuracy: 0.1395 - val_loss: 8.7172\n",
            "Epoch 22/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 94ms/step - accuracy: 0.2993 - loss: 3.2003 - val_accuracy: 0.1396 - val_loss: 8.8471\n",
            "Epoch 23/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 92ms/step - accuracy: 0.2970 - loss: 3.2324 - val_accuracy: 0.1431 - val_loss: 8.9035\n",
            "Epoch 24/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 91ms/step - accuracy: 0.3035 - loss: 3.1930 - val_accuracy: 0.1416 - val_loss: 9.0276\n",
            "Epoch 25/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 91ms/step - accuracy: 0.3115 - loss: 3.1462 - val_accuracy: 0.1400 - val_loss: 9.1420\n",
            "Epoch 26/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 91ms/step - accuracy: 0.3001 - loss: 3.2230 - val_accuracy: 0.1415 - val_loss: 9.2689\n",
            "Epoch 27/50\n",
            "\u001b[1m2002/2002\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 95ms/step - accuracy: 0.3014 - loss: 3.2080 - val_accuracy: 0.1398 - val_loss: 9.3061\n",
            "Epoch 28/50\n"
          ]
        }
      ],
      "source": [
        "# Define your model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "adam = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_temp, y_train_temp, epochs=50, validation_data=(X_val, y_val), verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuGqSBQ-0W3H"
      },
      "source": [
        "**Save Models (Weights and biases)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37cKxPZ-0auA"
      },
      "outputs": [],
      "source": [
        "# # Save model architecture as JSON file\n",
        "# from tensorflow.keras.models import model_from_json\n",
        "\n",
        "# model_json = model.to_json()\n",
        "# with open(\"lstm_model.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7RTzNGT0g6F"
      },
      "outputs": [],
      "source": [
        "# # Load model architecture from JSON file\n",
        "# from tensorflow.keras.models import model_from_json\n",
        "\n",
        "# with open(\"lstm_model.json\", \"r\") as json_file:\n",
        "#     loaded_model_json = json_file.read()\n",
        "\n",
        "# # Create model from loaded architecture\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# print(\"Model architecture loaded successfully from JSON file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVfp-l0l0kOM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdCS5Z9e0ofL"
      },
      "outputs": [],
      "source": [
        "# Plot Loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Woxxb_PlYlxx"
      },
      "outputs": [],
      "source": [
        "# Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mhFV_QrYyEt"
      },
      "source": [
        "Inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxGr7MJfYoxx"
      },
      "outputs": [],
      "source": [
        "def predict_top_five_words(model, tokenizer, seed_text):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    top_five_indexes = np.argsort(predicted[0])[::-1][:5]\n",
        "    top_five_words = []\n",
        "    for index in top_five_indexes:\n",
        "        for word, idx in tokenizer.word_index.items():\n",
        "            if idx == index:\n",
        "                top_five_words.append(word)\n",
        "                break\n",
        "    return top_five_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo3CHoGsY1ok"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "def predict_top_five_words(model, tokenizer, seed_text):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    top_five_indexes = np.argsort(predicted[0])[::-1][:5]\n",
        "    top_five_words = []\n",
        "    for index in top_five_indexes:\n",
        "        for word, idx in tokenizer.word_index.items():\n",
        "            if idx == index:\n",
        "                top_five_words.append(word)\n",
        "                break\n",
        "    return top_five_words\n",
        "\n",
        "def predict_and_display_top_five_words(seed_text, model, tokenizer):\n",
        "    top_five_words = predict_top_five_words(model, tokenizer, seed_text)\n",
        "    heading_app = f\"<h1>Sentence AutoCompletion App With Five Outputs</h1>\"\n",
        "    output_text = f\"<ul>{''.join([f'<li>{seed_text} {word}</li>' for word in top_five_words])}</ul>\"\n",
        "    javascript_code = f\"\"\"\n",
        "    <script>\n",
        "        var newWindow = window.open(\"\", \"_blank\");\n",
        "        newWindow.document.write('<html><head><title>Top Five Words</title></head><body>{heading_app} <br> <hr> {output_text}</body></html>');\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    return HTML(javascript_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf67L8YOY6cn"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "seed_text = \"She is my\"\n",
        "predict_and_display_top_five_words(seed_text, loaded_model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNhfBLA-ZAq3"
      },
      "outputs": [],
      "source": [
        "# Test 2:\n",
        "# Test the function\n",
        "seed_text = \"I have\"\n",
        "predict_and_display_top_five_words(seed_text, loaded_model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsOthDeMZGUq"
      },
      "outputs": [],
      "source": [
        "# Test 3:\n",
        "# Test the function\n",
        "seed_text = \"We love\"\n",
        "predict_and_display_top_five_words(seed_text, loaded_model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGW_4lMfZK5G"
      },
      "outputs": [],
      "source": [
        "# Test 3:\n",
        "seed_text = \"How are\"\n",
        "predict_and_display_top_five_words(seed_text, loaded_model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZXrca1oZOf9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}